{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People Car cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBEL_DEVICES=0 \\\n",
    "python -m torch.distributed.launch --nproc_per_node=1 \\\n",
    "CVPR/train_people_car_cl.py \\\n",
    "--csv-dir data/train_val_cl_20fold.csv \\\n",
    "--config-name 'config_eva_vit_people_car_cl' \\\n",
    "--image-size 224 \\\n",
    "--epochs 11 \\\n",
    "--init-lr 3e-5 \\\n",
    "--batch-size 32 \\\n",
    "--num-workers 8 \\\n",
    "--nbatch_log 300 \\\n",
    "--warmup_epochs 0 \\\n",
    "--fold 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People+Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "df1 = pd.read_csv('output/test_people_out.csv')\n",
    "df2 = pd.read_csv('output/test_car_out.csv')\n",
    "data = {\n",
    "    'results':[]\n",
    "}\n",
    "for idx, row in tqdm(df2.iterrows(), total=len(df1)):\n",
    "    res = {'text':row.prompt+'.', 'image_names':[]}\n",
    "    for i in range(10):\n",
    "        res['image_names'].append(row[f'img_{i}'].split('/')[-1])\n",
    "    data['results'].append(res)\n",
    "for idx, row in tqdm(df1.iterrows(), total=len(df1)):\n",
    "    res = {'text':row.prompt+'.', 'image_names':[]}\n",
    "    for i in range(10):\n",
    "        res['image_names'].append(row[f'img_{i}'].split('/')[-1])\n",
    "    data['results'].append(res)\n",
    "\n",
    "with open('output/submission.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### people only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "df1 = pd.read_csv('output/test_people_out.csv')\n",
    "df2 = pd.read_csv('output/test_car_out.csv')\n",
    "data = {\n",
    "    'results':[]\n",
    "}\n",
    "for idx, row in tqdm(df2.iterrows(), total=len(df1)):\n",
    "    res = {'text':row.prompt+'.', 'image_names':[]}\n",
    "    for i in range(10):\n",
    "        res['image_names'].append('nothing.jpg')\n",
    "    data['results'].append(res)\n",
    "for idx, row in tqdm(df1.iterrows(), total=len(df1)):\n",
    "    res = {'text':row.prompt+'.', 'image_names':[]}\n",
    "    for i in range(10):\n",
    "        res['image_names'].append(row[f'img_{i}'].split('/')[-1])\n",
    "    data['results'].append(res)\n",
    "\n",
    "with open('output/submission.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Car only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "df1 = pd.read_csv('output/test_people_out.csv')\n",
    "df2 = pd.read_csv('output/test_car_out.csv')\n",
    "data = {\n",
    "    'results':[]\n",
    "}\n",
    "for idx, row in tqdm(df2.iterrows(), total=len(df1)):\n",
    "    res = {'text':row.prompt+'.', 'image_names':[]}\n",
    "    for i in range(10):\n",
    "        res['image_names'].append(row[f'img_{i}'].split('/')[-1])\n",
    "    data['results'].append(res)\n",
    "for idx, row in tqdm(df1.iterrows(), total=len(df1)):\n",
    "    res = {'text':row.prompt+'.', 'image_names':[]}\n",
    "    for i in range(10):\n",
    "        res['image_names'].append('nothing.jpg')\n",
    "    data['results'].append(res)\n",
    "\n",
    "with open('output/submission.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dicts = torch.load('pretrained_weights/EVA02_CLIP_L_336_psz14_s6B_vision.pt')\n",
    "new_dicts = {}\n",
    "decoup_stage = 22\n",
    "for k, v in dicts.items():\n",
    "    if 'blocks.' in k:\n",
    "        stage = int(k.split('.')[1])\n",
    "        if stage < decoup_stage:\n",
    "            new_dicts[k.replace('blocks.', 'shared_blocks.')] = v\n",
    "        else:\n",
    "            for i in range(12):\n",
    "                new_dicts[f'decoupler.{i}.'+k.replace(str(stage), str(stage-decoup_stage))] = v\n",
    "    elif 'norm' in k or 'head' in k:\n",
    "        for i in range(12):\n",
    "            new_dicts[f'decoupler.{i}.'+k] = v\n",
    "    else:\n",
    "        new_dicts[k] = v\n",
    "torch.save(new_dicts, f'pretrained_weights/EVA02_CLIP_L_336_psz14_s6B_vision_decoupler_{decoup_stage}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dicts = torch.load('pretrained_weights/EVA02_CLIP_L_psz14_s4B_vision.pt')\n",
    "new_dicts = {}\n",
    "decoup_stage = 22\n",
    "for k, v in dicts.items():\n",
    "    if 'blocks.' in k:\n",
    "        stage = int(k.split('.')[1])\n",
    "        if stage < decoup_stage:\n",
    "            new_dicts[k.replace('blocks.', 'shared_blocks.')] = v\n",
    "        else:\n",
    "            for i in range(12):\n",
    "                new_dicts[f'decoupler.{i}.'+k.replace(str(stage), str(stage-decoup_stage))] = v\n",
    "    elif 'norm' in k or 'head' in k:\n",
    "        for i in range(12):\n",
    "            new_dicts[f'decoupler.{i}.'+k] = v\n",
    "    else:\n",
    "        new_dicts[k] = v\n",
    "torch.save(new_dicts, f'pretrained_weights/EVA02_CLIP_L_psz14_s4B_vision_decoupler_{decoup_stage}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc1fdf8315257af72404463f1ae84f6485d5212ca50b6e6157cecf4644ce291f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
